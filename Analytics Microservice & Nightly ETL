#!/usr/bin/env python3.12
"""
Nightly Spark ETL job:
- Reads raw event data (~5M rows)
- Computes aggregates (e.g., counts, averages)
- Applies k-anonymization (min group size = K)
- Writes locked Parquet for API consumption
"""
import os
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.window import Window
from dotenv import load_dotenv

# Load configuration
dotenv_path = os.getenv('DOTENV_PATH', '.env')
load_dotenv(dotenv_path)
RAW_DATA_PATH = os.getenv('RAW_DATA_PATH')
AGG_OUTPUT_PATH = os.getenv('AGG_OUTPUT_PATH')
K = int(os.getenv('K_ANON_THRESHOLD', '15'))
PHIPA_TAG = os.getenv('PHIPA_SECTION', 'PHIPA §13')

# Initialize Spark
spark = SparkSession.builder \
    .appName('NightlyAnalyticsETL') \
    .getOrCreate()

# Read raw data (example schema: user_id, event_type, timestamp, value)
df = spark.read.option('inferSchema', True) \
    .parquet(RAW_DATA_PATH)

# Compute aggregates: group by event_type, date
df = df.withColumn('date', F.to_date('timestamp'))
agg = df.groupBy('event_type', 'date') \
    .agg(
        F.count('*').alias('event_count'),
        F.avg('value').alias('avg_value')
    )

# Apply k-anonymization: drop groups smaller than K
group_counts = agg.select('event_type', 'date', 'event_count')
valid = group_counts.filter(F.col('event_count') >= K)
agg_k = agg.join(
    valid.select('event_type', 'date'),
    on=['event_type', 'date'], how='inner'
)

# Write anonymized aggregates
agg_k \
    .withColumn('phipa_tag', F.lit(PHIPA_TAG)) \
    .write \
    .mode('overwrite') \
    .parquet(AGG_OUTPUT_PATH)

spark.stop()

#!/usr/bin/env python3.12
"""
FastAPI application serving aggregated stats.
"""
import os
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import pandas as pd
from dotenv import load_dotenv

# Load configuration
dotenv_path = os.getenv('DOTENV_PATH', '.env')
load_dotenv(dotenv_path)
AGG_OUTPUT_PATH = os.getenv('AGG_OUTPUT_PATH')

app = FastAPI(
    title='Analytics API',
    version='1.0.0',
    description='Aggregated stats API with k-anonymization (k ≥ 15)'
)

data_df: Optional[pd.DataFrame] = None

class Stat(BaseModel):
    event_type: str
    date: str
    event_count: int
    avg_value: float
    phipa_tag: str

@app.on_event('startup')
def load_data():
    global data_df
    try:
        data_df = pd.read_parquet(AGG_OUTPUT_PATH)
    except Exception as e:
        raise RuntimeError(f'Failed to load aggregated data: {e}')

@app.get('/stats', response_model=List[Stat])
def get_all_stats():
    if data_df is None:
        raise HTTPException(status_code=503, detail='Data not loaded')
    return data_df.to_dict(orient='records')

@app.get('/stats/{event_type}', response_model=List[Stat])
def get_stats_by_event(event_type: str):
    if data_df is None:
        raise HTTPException(status_code=503, detail='Data not loaded')
    df = data_df[data_df['event_type'] == event_type]
    if df.empty:
        raise HTTPException(status_code=404, detail='No data for given event type')
    return df.to_dict(orient='records')

# Entry point for Uvicorn
def start():
    import uvicorn
    uvicorn.run('main:app', host='0.0.0.0', port=8000, reload=False)

if __name__ == '__main__':
    start()
